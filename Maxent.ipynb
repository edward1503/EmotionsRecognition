{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.48396794 0.5260521  0.50952859 0.51955868 0.53259779]\n",
      "Mean cross-validation score: 0.5143410190491314\n",
      "Accuracy: 0.529270248596632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.53      0.24      0.33        79\n",
      "     Disgust       0.53      0.59      0.56       257\n",
      "   Enjoyment       0.56      0.76      0.64       344\n",
      "        Fear       0.74      0.35      0.47        75\n",
      "       Other       0.39      0.36      0.38       238\n",
      "     Sadness       0.56      0.52      0.54       197\n",
      "    Surprise       0.64      0.25      0.35        57\n",
      "\n",
      "    accuracy                           0.53      1247\n",
      "   macro avg       0.56      0.44      0.47      1247\n",
      "weighted avg       0.53      0.53      0.52      1247\n",
      "\n",
      "Thời gian thực hiện: 1.4918832778930664 giây\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# # Kiểm tra sự tồn tại của các file\n",
    "# files = ['train_nor_811.xlsx', 'valid_nor_811.xlsx', 'test_nor_811.xlsx']\n",
    "# for file in files:\n",
    "#     if not os.path.exists(file):\n",
    "#         raise FileNotFoundError(f\"File {file} not found.\")\n",
    "\n",
    "# Đọc dữ liệu từ các file\n",
    "train_data = pd.read_excel('D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/train_nor_811.xlsx')\n",
    "valid_data = pd.read_excel('D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/valid_nor_811.xlsx')\n",
    "test_data = pd.read_excel('D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/test_nor_811.xlsx')\n",
    "\n",
    "# Kết hợp dữ liệu train và valid để tạo tập huấn luyện\n",
    "combined_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Chuyển về chữ thường\n",
    "    text = re.sub(r'\\W', ' ', text)  # Loại bỏ dấu câu\n",
    "    return text\n",
    "\n",
    "# Áp dụng tiền xử lý\n",
    "combined_data['Sentence'] = combined_data['Sentence'].apply(preprocess_text)\n",
    "\n",
    "# Chia dữ liệu thành đặc trưng và nhãn\n",
    "X = combined_data['Sentence']\n",
    "y = combined_data['Emotion']\n",
    "\n",
    "# Chia tập huấn luyện thành train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Chuyển đổi văn bản thành đặc trưng sử dụng TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Mã hóa nhãn\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Tạo mô hình MaxEnt (Logistic Regression)\n",
    "model = LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "\n",
    "# Thời gian bắt đầu huấn luyện\n",
    "start_time = time.time()\n",
    "model.fit(X_train_vectorized, y_train_encoded)\n",
    "\n",
    "# Đánh giá mô hình với cross-validation\n",
    "cv_scores = cross_val_score(model, X_train_vectorized, y_train_encoded, cv=5)\n",
    "print(\"Cross-validation scores:\", cv_scores)\n",
    "print(\"Mean cross-validation score:\", cv_scores.mean())\n",
    "\n",
    "# Dự đoán trên tập test\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "print(\"Accuracy:\", accuracy_score(y_test_encoded, y_pred))\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Dự đoán trên tập test\n",
    "test_vectorized = vectorizer.transform(test_data['Sentence'])\n",
    "test_pred = model.predict(test_vectorized)\n",
    "\n",
    "# Thêm dự đoán vào DataFrame\n",
    "test_data['Predicted Emotion'] = label_encoder.inverse_transform(test_pred)\n",
    "\n",
    "# Lưu kết quả ra file Excel\n",
    "test_data.to_excel('test_predictions.xlsx', index=False)\n",
    "\n",
    "# Lưu mô hình\n",
    "joblib.dump(model, 'emotion_model.pkl')\n",
    "\n",
    "# Thời gian thực hiện\n",
    "end_time = time.time()\n",
    "print(f\"Thời gian thực hiện: {end_time - start_time} giây\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/586.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------- 30.7/586.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 61.4/586.9 kB 656.4 kB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 174.1/586.9 kB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 327.7/586.9 kB 1.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 542.7/586.9 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 586.9/586.9 kB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results using TF-IDF features:\n",
      "Accuracy: 0.5714285714285714\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.52      0.35      0.41        49\n",
      "     Disgust       0.52      0.59      0.56       135\n",
      "   Enjoyment       0.69      0.78      0.73       214\n",
      "        Fear       0.72      0.42      0.53        31\n",
      "       Other       0.44      0.42      0.43       141\n",
      "     Sadness       0.52      0.59      0.55        86\n",
      "    Surprise       0.75      0.20      0.32        30\n",
      "\n",
      "    accuracy                           0.57       686\n",
      "   macro avg       0.59      0.48      0.50       686\n",
      "weighted avg       0.58      0.57      0.56       686\n",
      "\n",
      "\n",
      "Results using Word2Vec features:\n",
      "Accuracy: 0.4446064139941691\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.36      0.10      0.16        49\n",
      "     Disgust       0.38      0.48      0.42       135\n",
      "   Enjoyment       0.51      0.71      0.59       214\n",
      "        Fear       0.80      0.13      0.22        31\n",
      "       Other       0.42      0.26      0.32       141\n",
      "     Sadness       0.40      0.48      0.44        86\n",
      "    Surprise       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.44       686\n",
      "   macro avg       0.41      0.31      0.31       686\n",
      "weighted avg       0.43      0.44      0.41       686\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define preprocessing functions\n",
    "def preprocessEmoji(sentence):\n",
    "    emotion_dict = {\n",
    "        r'(:|;|=)+(\\)|\\]|>)+': '🙂',\n",
    "        r'(:|;|=)+(\\(|\\[|<)+': '😞',\n",
    "        r'(:|;|=)+(D|d)': '😁',\n",
    "        r'(-_-)|(-\\.-)': '😐',\n",
    "        r':v': '_pacman_smile_',\n",
    "        r'(:|;|=)+(\\'|`|\\\")+(\\)|\\]|>)+': '🥲',\n",
    "        r'(:|;|=)+(\\'|`|\\\")+(\\(|\\[|<)+': '😢',\n",
    "        r'@@': '😵‍💫',\n",
    "        r'đc': 'được',\n",
    "        r'đk': 'được',\n",
    "        r'bik': 'biết',\n",
    "        r'ngừi': 'người',\n",
    "        r'hix': 'hic',\n",
    "        r'lm': 'làm'\n",
    "    }\n",
    "    for key, value in emotion_dict.items():\n",
    "        sentence = re.sub(key, value, sentence)\n",
    "    sentence = emoji.demojize(sentence)\n",
    "    sentence = re.sub(r\":(.*?):\", r\" _\\1_ \", sentence)\n",
    "    sentence = re.sub(r'([!@#$%^&*()_+={};:\"\\'<>,/\\\\|~-])\\1+', r'\\1', sentence)\n",
    "    return sentence\n",
    "\n",
    "def tokenize(sentence):\n",
    "    start_token = ' _s_ '\n",
    "    end_token = ' _e_ '\n",
    "    sentence = sentence.lower()\n",
    "    sentence = preprocessEmoji(sentence)\n",
    "    sentence = start_token + sentence + end_token\n",
    "    return ' '.join(ViTokenizer.tokenize(sentence).split())\n",
    "\n",
    "# Apply preprocessing to all datasets\n",
    "def preprocess_data(data):\n",
    "    data = data.drop(columns=['Unnamed: 0'], errors='ignore')  # Drop unnecessary column if exists\n",
    "    data['Processed_Sentence'] = data['Sentence'].apply(tokenize)\n",
    "    return data\n",
    "\n",
    "# Word2Vec helper functions\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=1000):\n",
    "    if len(tokens_list) < 1:\n",
    "        return np.zeros(k)\n",
    "    vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    avg_vec = np.mean(vectorized, axis=0)\n",
    "    return avg_vec\n",
    "\n",
    "def get_word2vec_embeddings(vectors, sentences, k=1000):\n",
    "    return np.array([get_average_word2vec(sentence.split(), vectors, k=k) for sentence in sentences])\n",
    "\n",
    "# Load data\n",
    "file_paths = {\n",
    "    \"train\": 'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/train_nor_811.xlsx',\n",
    "    \"test\": 'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/valid_nor_811.xlsx',\n",
    "    \"valid\": 'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/test_nor_811.xlsx'\n",
    "}\n",
    "\n",
    "train_data = pd.read_excel(file_paths[\"train\"])\n",
    "test_data = pd.read_excel(file_paths[\"test\"])\n",
    "valid_data = pd.read_excel(file_paths[\"valid\"])\n",
    "\n",
    "# Preprocess datasets\n",
    "train_data = preprocess_data(train_data)\n",
    "test_data = preprocess_data(test_data)\n",
    "valid_data = preprocess_data(valid_data)\n",
    "\n",
    "# Vectorize text using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = vectorizer.fit_transform(train_data['Processed_Sentence'])\n",
    "X_test_tfidf = vectorizer.transform(test_data['Processed_Sentence'])\n",
    "\n",
    "# Word2Vec embeddings\n",
    "sentences = [sentence.split() for sentence in train_data['Processed_Sentence']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=1000, window=10, min_count=20, workers=100)\n",
    "X_train_w2v = get_word2vec_embeddings(word2vec_model.wv, train_data['Processed_Sentence'])\n",
    "X_test_w2v = get_word2vec_embeddings(word2vec_model.wv, test_data['Processed_Sentence'])\n",
    "\n",
    "y_train = train_data['Emotion']\n",
    "y_test = test_data['Emotion']\n",
    "\n",
    "# Train and evaluate MaxEnt model with TF-IDF features\n",
    "model_tfidf = LogisticRegression(C= 2.0 ,solver='lbfgs',max_iter=7000)\n",
    "model_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate TF-IDF model\n",
    "y_pred_tfidf = model_tfidf.predict(X_test_tfidf)\n",
    "print(\"Results using TF-IDF features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_tfidf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_tfidf))\n",
    "\n",
    "# Train and evaluate MaxEnt model with Word2Vec features\n",
    "model_w2v = LogisticRegression(C= 700.0,solver='lbfgs',max_iter=7000)\n",
    "model_w2v.fit(X_train_w2v, y_train)\n",
    "\n",
    "# Evaluate Word2Vec model\n",
    "y_pred_w2v = model_w2v.predict(X_test_w2v)\n",
    "print(\"\\nResults using Word2Vec features:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_w2v))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_w2v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import re\n",
    "from pyvi import ViTokenizer\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 59ms/step - accuracy: 0.1982 - loss: -46.1175\n",
      "Epoch 2/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1909 - loss: -457.3328\n",
      "Epoch 3/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1879 - loss: -1399.7852\n",
      "Epoch 4/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1945 - loss: -2793.3647\n",
      "Epoch 5/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1918 - loss: -4788.7822\n",
      "Epoch 6/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1833 - loss: -7258.2212\n",
      "Epoch 7/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1866 - loss: -9931.4639\n",
      "Epoch 8/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1893 - loss: -13398.0850\n",
      "Epoch 9/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.1818 - loss: -16757.1582\n",
      "Epoch 10/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1842 - loss: -21108.9570\n",
      "Epoch 11/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1928 - loss: -25217.3340\n",
      "Epoch 12/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1947 - loss: -29315.0957\n",
      "Epoch 13/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1862 - loss: -34736.1055\n",
      "Epoch 14/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1868 - loss: -40930.1836\n",
      "Epoch 15/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1880 - loss: -45850.9609\n",
      "Epoch 16/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1958 - loss: -51987.0391\n",
      "Epoch 17/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1967 - loss: -57130.7656\n",
      "Epoch 18/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1868 - loss: -63317.7812\n",
      "Epoch 19/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1879 - loss: -71661.1016\n",
      "Epoch 20/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1922 - loss: -80167.4141\n",
      "Epoch 21/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1870 - loss: -86509.0156\n",
      "Epoch 22/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1779 - loss: -94857.4844\n",
      "Epoch 23/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1834 - loss: -100858.6562\n",
      "Epoch 24/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1851 - loss: -109755.7031\n",
      "Epoch 25/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1816 - loss: -121361.6406\n",
      "Epoch 26/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1947 - loss: -128437.4453\n",
      "Epoch 27/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.1959 - loss: -134471.7812\n",
      "Epoch 28/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1936 - loss: -142665.6562\n",
      "Epoch 29/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1844 - loss: -155278.4375\n",
      "Epoch 30/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1921 - loss: -163541.3594\n",
      "Epoch 31/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1871 - loss: -177964.3750\n",
      "Epoch 32/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1846 - loss: -188364.8125\n",
      "Epoch 33/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1987 - loss: -194599.0156\n",
      "Epoch 34/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 59ms/step - accuracy: 0.1929 - loss: -205535.0156\n",
      "Epoch 35/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 105ms/step - accuracy: 0.1898 - loss: -217323.1406\n",
      "Epoch 36/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1936 - loss: -229299.6094\n",
      "Epoch 37/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1991 - loss: -240164.2500\n",
      "Epoch 38/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1889 - loss: -256709.7344\n",
      "Epoch 39/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1870 - loss: -267044.3750\n",
      "Epoch 40/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1942 - loss: -275189.1250\n",
      "Epoch 41/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1897 - loss: -286360.3438\n",
      "Epoch 42/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1804 - loss: -309498.0000\n",
      "Epoch 43/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1856 - loss: -320264.7500\n",
      "Epoch 44/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1970 - loss: -321857.9375\n",
      "Epoch 45/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1888 - loss: -338043.8750\n",
      "Epoch 46/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - accuracy: 0.1857 - loss: -356340.2188\n",
      "Epoch 47/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 91ms/step - accuracy: 0.1823 - loss: -374907.7812\n",
      "Epoch 48/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1869 - loss: -389255.2188\n",
      "Epoch 49/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1864 - loss: -394509.2188\n",
      "Epoch 50/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.1985 - loss: -401743.1875\n",
      "Epoch 51/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 85ms/step - accuracy: 0.1904 - loss: -424814.4375\n",
      "Epoch 52/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1962 - loss: -433829.8438\n",
      "Epoch 53/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1895 - loss: -464299.8750\n",
      "Epoch 54/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.1972 - loss: -468586.8750\n",
      "Epoch 55/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.1896 - loss: -481187.8750\n",
      "Epoch 56/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1865 - loss: -501884.7812\n",
      "Epoch 57/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.1891 - loss: -514688.6875\n",
      "Epoch 58/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 85ms/step - accuracy: 0.1941 - loss: -534339.8125\n",
      "Epoch 59/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1954 - loss: -539744.3750\n",
      "Epoch 60/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1798 - loss: -572381.9375\n",
      "Epoch 61/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 58ms/step - accuracy: 0.1991 - loss: -577316.7500\n",
      "Epoch 62/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 83ms/step - accuracy: 0.1978 - loss: -587901.1250\n",
      "Epoch 63/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.1926 - loss: -619503.0000\n",
      "Epoch 64/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1913 - loss: -618356.0625\n",
      "Epoch 65/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.1885 - loss: -664627.4375\n",
      "Epoch 66/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.1852 - loss: -671535.6250\n",
      "Epoch 67/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.2080 - loss: -669808.0625\n",
      "Epoch 68/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 72ms/step - accuracy: 0.1921 - loss: -713785.6875\n",
      "Epoch 69/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1940 - loss: -728952.6250\n",
      "Epoch 70/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 57ms/step - accuracy: 0.1938 - loss: -732788.1875\n",
      "Epoch 71/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 64ms/step - accuracy: 0.1877 - loss: -766783.0625\n",
      "Epoch 72/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 95ms/step - accuracy: 0.1866 - loss: -795468.7500\n",
      "Epoch 73/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.1898 - loss: -802761.3125\n",
      "Epoch 74/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1950 - loss: -806060.5000\n",
      "Epoch 75/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 93ms/step - accuracy: 0.1864 - loss: -832381.6250\n",
      "Epoch 76/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 95ms/step - accuracy: 0.1861 - loss: -858872.0625\n",
      "Epoch 77/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.1897 - loss: -888155.6875\n",
      "Epoch 78/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1924 - loss: -909518.9375\n",
      "Epoch 79/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 91ms/step - accuracy: 0.1872 - loss: -935864.3125\n",
      "Epoch 80/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1977 - loss: -954120.4375\n",
      "Epoch 81/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1836 - loss: -980361.3125\n",
      "Epoch 82/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1896 - loss: -1000009.1250\n",
      "Epoch 83/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.1803 - loss: -1017638.0000\n",
      "Epoch 84/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1904 - loss: -1043067.3750\n",
      "Epoch 85/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 97ms/step - accuracy: 0.1960 - loss: -1043174.6250\n",
      "Epoch 86/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 100ms/step - accuracy: 0.1858 - loss: -1077523.6250\n",
      "Epoch 87/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 67ms/step - accuracy: 0.1880 - loss: -1109777.7500\n",
      "Epoch 88/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.1823 - loss: -1121257.8750\n",
      "Epoch 89/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.1910 - loss: -1175631.8750\n",
      "Epoch 90/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 92ms/step - accuracy: 0.1995 - loss: -1174783.8750\n",
      "Epoch 91/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 96ms/step - accuracy: 0.1862 - loss: -1186482.5000\n",
      "Epoch 92/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 88ms/step - accuracy: 0.1949 - loss: -1195693.0000\n",
      "Epoch 93/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1921 - loss: -1218964.3750\n",
      "Epoch 94/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1960 - loss: -1260281.0000\n",
      "Epoch 95/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1993 - loss: -1262548.2500\n",
      "Epoch 96/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1931 - loss: -1298079.0000\n",
      "Epoch 97/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 89ms/step - accuracy: 0.1863 - loss: -1332363.8750\n",
      "Epoch 98/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 90ms/step - accuracy: 0.1943 - loss: -1324576.7500\n",
      "Epoch 99/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 86ms/step - accuracy: 0.1871 - loss: -1415177.7500\n",
      "Epoch 100/100\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 87ms/step - accuracy: 0.1874 - loss: -1404428.5000\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 37ms/step - accuracy: 0.2185 - loss: -1333983.1250\n",
      "Precision Accuracy with LSTM: 20.61%\n"
     ]
    }
   ],
   "source": [
    "def preprocessEmoji(sentence):\n",
    "    emotion_dict = {\n",
    "        r'(:|;|=)+(\\)|]|>)+': '🙂', \n",
    "        r'(:|;|=)+(\\(|\\[|<)+': '😞', \n",
    "        r'(:|;|=)+(D|d)': '😁',\n",
    "        r'(-_-)|(-\\.-)': '😐',\n",
    "        r':v': '_pacman_smile_',\n",
    "        r'(:|;|=)+(\\'|`|\\\")+(\\)|]|>)+': '🥲', \n",
    "        r'(:|;|=)+(\\'|`|\\\")+(\\(|\\[|<)+': '😢',\n",
    "        r'@@': '😵‍💫',\n",
    "        r'\\bđc\\b': 'được',\n",
    "        r'\\bđk\\b': 'được',\n",
    "        r'\\bbik\\b': 'biết',\n",
    "        r'\\bngừi\\b': 'người',\n",
    "        r'\\bhix\\b': 'hic',\n",
    "        r'\\blm\\b': 'làm'\n",
    "    }\n",
    "    for key, value in emotion_dict.items():\n",
    "        sentence = re.sub(key, value, sentence)\n",
    "    sentence = emoji.demojize(sentence)\n",
    "    sentence = re.sub(r\":(.*?):\", r\" _\\1_ \", sentence)\n",
    "    sentence = re.sub(r'([!@#$%^&*()_+={};:\"\\'<>,?/\\|~-])\\1+', r'\\1', sentence)\n",
    "    return sentence\n",
    "\n",
    "def tokenize(sentence):\n",
    "    start_token = ' _s_ '\n",
    "    end_token = ' _e_ '\n",
    "    sentence = sentence.lower()\n",
    "    sentence = preprocessEmoji(sentence)\n",
    "    sentence = start_token + sentence + end_token\n",
    "    return ' '.join(ViTokenizer.tokenize(sentence).split())\n",
    "\n",
    "# Kiểm tra sự tồn tại của các file\n",
    "files = [\n",
    "    'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/train_nor_811.xlsx',\n",
    "    'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/valid_nor_811.xlsx',\n",
    "    'D:/HK5/NLP/UIT-VSMEC-20241212T141641Z-001/UIT-VSMEC/test_nor_811.xlsx'\n",
    "]\n",
    "\n",
    "for file in files:\n",
    "    if not os.path.exists(file):\n",
    "        raise FileNotFoundError(f\"File {file} not found.\")\n",
    "\n",
    "# Đọc dữ liệu từ các file\n",
    "train_data = pd.read_excel(files[0])\n",
    "valid_data = pd.read_excel(files[1])\n",
    "test_data = pd.read_excel(files[2])\n",
    "\n",
    "# Kết hợp dữ liệu train và valid để tạo tập huấn luyện\n",
    "combined_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "\n",
    "# Tiền xử lý dữ liệu\n",
    "combined_data['Sentence'] = combined_data['Sentence'].apply(tokenize)\n",
    "\n",
    "# Chia dữ liệu thành đặc trưng và nhãn\n",
    "X = combined_data['Sentence']\n",
    "y = combined_data['Emotion']\n",
    "\n",
    "# Chia tập huấn luyện thành train và test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Chuyển đổi nhãn thành số\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Sử dụng Tokenizer để chuyển đổi câu thành vector\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Chuyển đổi văn bản thành số\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Padding để đảm bảo tất cả các câu có cùng độ dài\n",
    "max_length = max([len(seq) for seq in X_train_seq])\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Xây dựng mô hình LSTM\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=max_length))\n",
    "model_lstm.add(LSTM(128, return_sequences=True))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(LSTM(64))\n",
    "model_lstm.add(Dropout(0.2))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dense(1, activation='sigmoid'))  # Thay đổi kích hoạt nếu cần cho bài toán phân loại\n",
    "\n",
    "# Biên dịch mô hình\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Huấn luyện mô hình\n",
    "model_lstm.fit(X_train_pad, y_train_encoded, epochs=100, batch_size=32)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "accuracy = model_lstm.evaluate(X_test_pad, y_test_encoded)\n",
    "print(f'Precision Accuracy with LSTM: {accuracy[1] * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
